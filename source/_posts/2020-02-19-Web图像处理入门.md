---
title: Web图像处理入门
categories:
  - JS图形学与H5游戏
  - null
tags:
  - null
  - null
date: 2020-02-19 07:18:50
top:
---



# Canvas 2D
## 一.认识像素数据
``` bash
// 从 x=0，y=0 开始，取宽=2，高=2 的像素数据
let imageData = ctx.getImageData(0, 0, 2, 2)
console.log(imageData);
```

getImageData 获取图片像素数据，返回ImageData 对象，是拷贝了画布指定矩形的像素数据，如下图

![](1.jpeg)

imageData.data 中有四个(宽 x 高=2x2=4)像素的数据，每个像素数据，都存在着四方面的信息，即 RGBA 值
R - 红色 (0-255; 0 是黑色，255 是纯红色)
G - 绿色 (0-255; 0 是黑色，255 是纯绿色)
B - 蓝色 (0-255; 0 是黑色的，255 是纯蓝色)
A – 透明度 (0-255; 0 是透明的，255 是完全可见不透明的)

## 二.常见滤镜处理
常见滤镜有灰色、颜色反转、黑白、马赛克、锐化、增强饱和度等等
我们在PS和各种各样的美图拍照APP中都比较常看到
以这个小狗为例子

![](2.jpeg)

1.单色滤镜
比如实现红色滤镜
顾名思义，就是只保留红色值不变，把绿色和蓝色去除掉（值设为 0）

``` bash
// 滤镜函数 - 红色滤镜
function filter (imageData, ctx) {
    let imageData_length = imageData.data.length / 4; // 4 个为一个像素
    for (let i = 0; i < imageData_length; i++) {
        // imageData.data[i * 4 + 0] = 0;  // 红色值不变
        imageData.data[i * 4 + 1] = 0; // 绿色值设置为 0
        imageData.data[i * 4 + 2] = 0; // 蓝色值设置为 0
    }
    return imageData;
}
```
效果如下：
![](3.webp)

2.灰色滤镜
黑白照片效果，将颜色的 RGB 设置为相同的值即可使得图片为灰色，我们可以取三个色值的平均值。
注意，在严格意义上，灰度化既不是用 R 通道覆盖 RGB，也不是对 RGB 通道简单取平均，而需要一个比例系数。这里为入门做了简化

```bash
// 滤镜函数 - 灰色滤镜
function filter (imageData, ctx) {
    let imageData_length = imageData.data.length / 4; // 4 个为一个像素
    for (let i = 0; i < imageData_length; i++) {
        let newColor = (imageData.data[i * 4] + imageData.data[i * 4 + 1] + imageData.data[i * 4 + 2]) / 3;
        imageData.data[i * 4 + 0] = newColor;
        imageData.data[i * 4 + 1] = newColor;
        imageData.data[i * 4 + 2] = newColor;
    }
    return imageData;
}
```
效果如下：
![](4.webp)

3.反向滤镜
RGB 三种颜色分别取 255 的差值

```bash
// 滤镜函数 - 反向滤镜
function filter (imageData, ctx) {
    let imageData_length = imageData.data.length / 4; // 4 个为一个像素
    for (let i = 0; i < imageData_length; i++) {
        imageData.data[i * 4 + 0] = 255 - imageData.data[i * 4];
        imageData.data[i * 4 + 1] =  255 - imageData.data[i * 4 + 1];
        imageData.data[i * 4 + 2] =  255 - imageData.data[i * 4 + 2];
    }
    return imageData;
}
```
效果如下：
![](5.webp)

上面三种滤镜，都是通过控制每个像素 4 个数据的值，即可达到简单滤镜的效果。
但是复杂的滤镜比如边缘检测，就需要用到卷积运算来实现。

## 三．卷积
卷积是一个常用的图像处理技术。卷积操作是使用一个卷积核（kernel）对图像中的每一个像素进行一些列操作，可以改变像素强度。使用卷积技术，你可以获取一些流行的图像效果，比如边缘检测、锐化、模糊、浮雕等。

比如下图，是通过卷积运算后，输出的边缘检测图像效果；如果通过上面简单滤镜算法，我们很难找到物体的边缘！
![](6.webp)

### 1.卷积计算过程
![](7.webp)
卷积运算是使用一个卷积核对输入图像中的每个像素进行一系列四则运算。
卷积核（算子）是用来做图像处理时的矩阵，通常为 3x3 矩阵。
使用卷积进行计算时，需要将卷积核的中心放置在要计算的像素上，一次计算核中每个元素和其覆盖的图像像素值的乘积并求和，得到的结构就是该位置的新像素值。

为了更形象，附上动图
![](8.gif)

计算步骤如下：
1、我们使用 3×3 的卷积核，将其覆盖在输入图像，对应的数字相乘，最后全部相加，即可得到第一个输出数据；
2、把 3×3 的卷积核右移一格；
3、重复 1 的计算过程，得到第二个数据；
4、重复以上过程。

按照我们上面讲的图片卷积，如果原始图片尺寸为 6 x 6，卷积核尺寸为 3 x 3，则卷积后的图片尺寸为(6-3+1) x (6-3+1) = 4 x 4，卷积运算后，输出图片尺寸缩小了，这显然不是我们想要的结果！

为了解决这个问题，可以使用 padding 方法，即把原始图片尺寸进行扩展，扩展区域补零，扩展尺寸为卷积核的半径（3x3 卷积核半径为 1，5x5 卷积核半径为 2）。
![](9.webp)

一个尺寸 6 x 6 的数据矩阵，经过 padding 后，尺寸变为 8 * 8，卷积运算后输出尺寸为 6 x 6，保证了图片尺寸不变化。

### 2.卷积核特性
{% blockquote %}
1.大小应该是奇数，这样它才有一个中心，例如 3x3，5x5 或者 7x7。
2.卷积核上的每一位乘数被称为权值，它们决定了这个像素的分量有多重。
3.它们的总和加起来如果等于 1，计算结果不会改变图像的灰度强度。
4.如果大于 1，会增加灰度强度，计算结果使得图像变亮。
5.如果小于 1，会减少灰度强度，计算结果使得图像变暗。
6.如果和为 0，计算结果图像不会变黑，但也会非常暗。
{% endblockquote %}


### 3.边缘检测
常用于检测物体边缘的卷积核是一个中间是 8，周围是-1 的 3x3 数据矩阵。
![](10.webp)

我们能感受到物体的边缘，是因为边缘有明显的色差。假设输入图像的部分色值为 10，部分色值为 50，那么 10 和 50 之间就存在色差，边缘就在这个地方。

经过卷积计算之后，我们可以看到色值相同的部分都变成了 0 表现为黑色，只有边缘的色值计算结果大于 0（色值最小是 0，负数色值也是黑色），即色值为 120 的边缘就凸显出来了！代码如下：

```bash
// 卷积计算函数
function convolutionMatrix(output, input, kernel) {
    let w = input.width, h = input.height;
    let iD = input.data, oD = output.data;
    for (let y = 1; y < h - 1; y += 1) {
        for (let x = 1; x < w - 1; x += 1) {
            for (let c = 0; c < 3; c += 1) {
                let i = (y * w + x) * 4 + c;
                oD[i] = kernel[0] * iD[i - w * 4 - 4] +
                        kernel[1] * iD[i - w * 4] +
                        kernel[2] * iD[i - w * 4 + 4] +
                        kernel[3] * iD[i - 4] +
                        kernel[4] * iD[i] +
                        kernel[5] * iD[i + 4] +
                        kernel[6] * iD[i + w * 4 - 4] +
                        kernel[7] * iD[i + w * 4] +
                        kernel[8] * iD[i + w * 4 + 4];
            }
            oD[(y * w + x) * 4 + 3] = 255;
        }
    }
    return output;
}

// 滤镜函数
function filter (imageData, ctx) {
    let kernel = [-1, -1, -1,
                  -1, 8, -1,
                  -1, -1, -1]; // 边缘检测卷积核
    return convolutionMatrix(ctx.createImageData(imageData), imageData, kernel);
}

```

### 4.锐化
我们只要使用不同的卷积核就能得到不同的图像处理效果，比如使用下面这个卷积核，就能得到锐化效果

```bash
let kernel = [-1, -1, -1,
              -1, 9, -1,
              -1, -1, -1]; // 锐化卷积核
```
![](11.webp)

锐化也是一种针对边缘处理（增强）的效果
前面有提到“卷积核的总和加起来如果等于 1，计算结果不会改变图像的灰度强度”。
所以只要把边缘检测卷积核中间的 8 改为 9，就能实现边缘增强的锐化效果！

# WebGL
## 一．WebGL概念入门
### 1.Shader
Shader 着色器，是存放 <font color="#FF0000"> 图形算法 </font> 的对象。
相比于在 CPU 上单线程执行的 JS 代码，着色器在 GPU 上并行执行，计算出每帧数百万个像素各自的颜色。

WebGL 标准里，分为顶点着色器和片元着色器。
你可以把它们想象成两个需要你写 C-style 代码，跑在 GPU 上的函数。它们大体上分别做这样的工作：

{% blockquote %}
顶点着色器输入原始的顶点坐标，输出经过你计算出的坐标。
片元着色器输入一个像素位置，输出根据你计算出的像素颜色。
{% endblockquote %}

### 2.Resource
Resource 资源，是存放  <font color="#FF0000"> 图形数据 </font> 的对象。
就像 JSON 成为 Web App 的数据那样，资源是传递给着色器的数据，包括大段的顶点数组、纹理图像，以及全局的配置项等。

下图的 Buffers / Textures / Uniforms 都属于典型的资源

### 3.Draw
Draw 绘制，是选好资源后运行着色器的请求。要想渲染真实的场景，一般需要多组着色器与多个资源，来回绘制多次才能完成一帧。每次绘制前，我们都需要选好着色器，并为其关联好不同的资源，也都会启动一次图形渲染管线。

### 4.Command
Command 命令，是执行绘制前的配置。
WebGL 是非常有状态的。每次绘制前，我们都必须小心地处理好状态机。这些状态变更就是通过命令来实现的。

这些概念是如何协同工作的呢？请看下图：
![](12.png)

### 5.渲染管线
渲染管线，一般指的就是GPU上由顶点数据到像素的过程。
对现代 GPU 来说，管线中的某些阶段是可编程的。

### 6.补充注意
（1）注意区分 WebGL 中的顶点和坐标概念。
顶点 (vertex) 不仅可以包含一个点的坐标属性，还可以包含法向量、颜色等其它属性。
这些属性都可以输入顶点着色器中来做计算。

## 二．Shader着色器
由上面的概念我们很容易知道，要对图形进行处理的算法就是写在shader着色器中

### 1.webgl标准的GLSL语言
GLSL即 OpenGL Shading Language，着色器编程语言，
是为图形计算量身定制的，它包含一些针对向量和矩阵操作的有用特性，
是和GPU打交道的语言，
GLSL的语法比较像C语言,其实就是C语言的变体

很多常见的webGL库都进行了封装，使用他们提供的api方便我们编写shader
比如threejs的ShaderMaterial或RawShaderMaterial

### 2.看个简单的demo
比如我们绘制一个三角形，shader如下
一个顶点着色器，一个片元着色器

```bash
const vertexShader = `
attribute vec4 position;
attribute vec4 color;
varying highp vec4 vColor;
void main() {
  vColor = color;
  gl_Position = position;
}
`
const fragmentShader = `
varying highp vec4 vColor;
void main() {
  gl_FragColor = vColor;
}
`
```

（1）vec4 则是GLSL内置的 4 维向量数据类型。
（2）在 WebGL 中，顶点着色器将 gl_Position 变量作为坐标位置输出
片元着色器则将 gl_FragColor 变量作为像素颜色输出。
本例中的顶点和片元着色器，执行的都只是最简单的赋值操作。
（3）名为 vColor 的 varying 变量，会由顶点着色器传递到片元着色器，并自动插值。
最终三角形在顶点位置呈现我们定义的红绿蓝纯色，而其他位置则被渐变填充，这就是插值计算的结果。
（4）变量前的 highp 修饰符用于指定精度，
也可以在着色器最前面加一行 precision highp float 来省略为每个变量手动指定精度。在现在这个时代，基本可以一律用高精度了。
（5）真正的渲染算法，其实只有片元着色器里的这一行：

```bash
void main() {
  gl_FragColor = vColor;
}
```
对每个像素，这个 main 函数都会执行，将插值后的 varying 变量 vColor 颜色直接赋给 gl_FragColor 作为输出。

玩点别的花样

```bash
gl_FragColor = vec4(0.8, 0.9, 0.6, 0.4); // 固定颜色
gl_FragColor = vColor.xyzw; // 四个分量的语法糖
gl_FragColor = vColor.rgba; // 四个分量的等效语法糖
gl_FragColor = vColor.stpq; // 四个分量的等效语法糖
gl_FragColor = vColor + vec4(0.5); // 变淡
gl_FragColor = vColor * 0.5; // 变暗
gl_FragColor = vColor.yxzw; // 交换 X 与 Y 分量
gl_FragColor = vColor.rbga; // 交换 G 与 B 分量
gl_FragColor = vColor.rrrr; // 灰度展示 R 分量
gl_FragColor = vec4(vec2(0), vColor.ba); // 清空 R 与 G 分量
```
虽然这些例子只示范了 GLSL 的基本语法，但别忘了这可是编译到 GPU 上并行计算的代码，和单线程的 JS 有很大的差别。
只不过，目前我们的输入都是由各顶点之间的颜色插值而来，因此效果就是普通的渐变效果。下面介绍怎样渲染出常见的点阵图像

## 三.webGL渲染图像
### 1.整体的渲染逻辑
（1）初始化着色器、矩形资源和纹理资源
（2）异步加载图像，完成后把图像设置为纹理
（3）执行绘制

### 2.着色器代码分析
```bash
const vs = `
attribute vec4 position;
attribute vec2 texCoord;
varying highp vec2 vTexCoord;

void main() {
  vTexCoord = texCoord;
  gl_Position = position;
}
`

const fs = `
varying highp vec2 vTexCoord;
uniform sampler2D img;

void main() {
  gl_FragColor = texture2D(img, vTexCoord);
}
`
```
像 vColor 那样地，我们将 vTexCoord 变量从顶点着色器传入了片元着色器，这时同样隐含了插值处理。
这组着色器中，最关键的是这么两行：

```bash
uniform sampler2D img;
// ...
gl_FragColor = texture2D(img, vTexCoord);
```

片元着色器中 uniform sampler2D 类型的 img 变量，会被绑定到一张图像纹理上。
然后，我们就可以用 WebGL 内置的 texture2D 函数来做纹理采样了。
因此，这个着色器的渲染算法，其实就是采样 img 图像的 vTexCoord 位置，将获得的颜色作为该像素的输出。
对整个矩形内的每个像素点都执行一遍这个采样过程后，自然就把图像搬上屏幕了。

## 四.为图像增加滤镜
现在，图像的采样过程已经处于我们的着色器代码控制之下了。
这意味着我们可以轻易地控制每个像素的渲染算法，实现图像滤镜。

### 1.灰度滤镜
简单的RGB通道设置一样，或者取个平均值

```bash
// 先采样出纹理的 vec4 颜色
vec4 texColor = texture2D(img, vTexCoord);

// 然后可以这样，RGB通道都设置为R通道的值
gl_FragColor = texColor.rrra;

// 或者取个平均值
float average = (texColor.r + texColor.g + texColor.b) / 3.0;
gl_FragColor = vec4(vec3(average), texColor.a);
```
同样要知道，严格意义上，灰度化既不是用 R 通道覆盖 RGB，也不是对 RGB 通道简单取平均，而需要一个比例系数。这里为入门做了简化

2.饱和度滤镜
```bash
precision highp float;
uniform sampler2D img;
varying vec2 vTexCoord;

const float saturation = 0.5; // 饱和度比例常量

void main() {
  vec4 color = texture2D(img, vTexCoord);
  float average = (color.r + color.g + color.b) / 3.0;
  if (saturation > 0.0) {
    color.rgb += (average - color.rgb) * (1.0 - 1.0 / (1.001 - saturation));
  } else {
    color.rgb += (average - color.rgb) * (-saturation);
  }
  gl_FragColor = color;
}
```

五.叠加多个图像
shader如下

```bash
const fs = `
precision highp float;
uniform sampler2D img0;
uniform sampler2D img1;
varying vec2 vTexCoord;

void main() {
  vec4 color0 = texture2D(img0, vTexCoord);
  vec4 color1 = texture2D(img1, vTexCoord);
  gl_FragColor = color0 * color1.r;
}
`
```

核心代码在于 gl_FragColor = color0 * color1.r;
这一句，而这两个颜色则分别来自于对两张图像的 texture2D 采样。
有了更丰富的输入，我们自然可以有更多的变化可以玩了。比如这样：

```bash
gl_FragColor = color0 * (1.0 - color1.r);
```

我们就可以得到相反的叠加效果了

## 六.组合多个滤镜，webGL的离屏渲染
## 1.两种方案
到现在为止我们已经单独实现过多种滤镜了，但如何将它们的效果串联起来呢？

WebGL 的着色器毕竟是字符串，我们可以做 魔改拼接 ，生成不同的着色器。
这确实是许多 3D 库中的普遍实践，也利于追求极致的性能。

但也可以选择另一种工程上实现更为简洁优雅的方式，即 离屏的链式渲染 。

## 2.离屏渲染
假设我们有 A B C 等多种滤镜（即用于图像处理的着色器），那么该如何将它们的效果依次应用到图像上呢？我们需要先为原图应用滤镜 A，然后将 A 的渲染结果传给 B，再将 A + B 的渲染结果传给 C…依此类推，即可组成一条完整的滤镜链。

为了实现这一目标，我们显然需要暂存某次渲染的结果。
熟悉 Canvas 的同学一定对离屏渲染不陌生，在 WebGL 中也有类似的概念。
但 WebGL 的离屏渲染，并不像 Canvas 那样能直接新建多个离屏的 canvas标签，而是以渲染到纹理的方式来实现的。

（1）注意一个webGL和OpenGL体系中的命名槽点，Buffer 和 Framebuffer
完全是两个东西
Buffer 可以理解为存储大段有序数据的对象，
而 Framebuffer 指代的则是屏幕！
（2）我们渲染到屏幕时，使用的就是默认的物理 Framebuffer。
但离屏渲染时，我们渲染的 Framebuffer 是个虚拟的对象，即所谓的 Framebuffer Object (FBO)。纹理对象可以 attach 到 Framebuffer Object 上，这样绘制时就会将像素数据写到内存，而不是物理显示设备了，实现了离屏渲染。

简化下步骤
{% blockquote %}
1.离屏渲染时，要将渲染目标从物理 Framebuffer 换成 FBO。
2.FBO 只是个壳，要将纹理对象挂载上去，这才是像素真正写入的地方。
{% endblockquote %}

## 七.总结
（1）从canvas和webGL两大类的图形处理中我们可以发现
拿到图片的像素点数据之后，具体的滤镜处理算法都是类似，更多的滤镜算法也可以自行去查阅
（2）而获取像素点数据的方法，二者就不相同
canvas直接提供了api给我们，可以很方便的获取像素点数据
而webGL，要么我们使用各种成熟的库提供的api，要么我们自己写webGL底层的GLSL代码来实现

了解下底层的实现也是一种学习快乐~

参考文献
《前端图像处理之滤镜》
《实用 WebGL 图像处理入门》

同时欢迎关注我的个人微信公众号：
{% img https://www.xuanbiyijue.com//img/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BA%8C%E7%BB%B4%E7%A0%81.jpg 300 300 %}